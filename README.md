# Observation Lakehouse

> **TL;DR** - A data-lakehouse implementation that stores **stimulus-response observations** from controlled experiments ([LASSO arena](https://softwareobservatorium.github.io/)) **and** from real-world CI pipelines.  It provides the first practical, scalable realization of the **continual Stimulusâ€‘Response Cube (SRC, formlery SRH)** introduced in our TOSEM article [*"Morescient GAI for Software Engineering"*](https://dl.acm.org/doi/10.1145/3709354).


## Overview

**Observation Lakehouse** is a Python library that:

* **Ingests** raw LASSO arena runs (or any CI-pipeline output) into three Iceberg-managed tables - `observations`, `tests`, and `code_implementations`.
* Stores the data as **partitioned Parquet files** (`data_set_id`, `problem_id`) and keeps full Iceberg metadata for ACID guarantees and schema evolution.
* Provides a **single-query interface** (DuckDB + Arrow) to materialise:
  * **SRM output views** (the "stimulus-response matrix" for a given coding problem),
  * **Behavioural clustering** (equivalence classes of implementations),
  * **Consensus oracles**, (majority voting)
  * **Three-way joins** across all three tables.
* Achieves **~155k records/s ingestion** and **sub-100-ms interactive query latency** on a local laptop.

The repository contains everything needed to **reproduce** the performance assessment, **extend** the platform with new datasets, ideas etc.


## Project Structure

```
observation-lakehouse/
â”œâ”€â”€ olake/                         # Main Python package
â”‚   â”œâ”€â”€ ingest/                    # Data-ingestion helpers
â”‚   â”‚   â”œâ”€â”€ arena.py               # Utilities to ingest data from LASSO arena
â”‚   â”œâ”€â”€ lakehouse.py               # Main lakehouse implementation
â”œâ”€â”€ notebooks/                     # Jupyter Notebooks
â”‚   â”œâ”€â”€ analysis.ipynb             # End-to-end query walkthrough
â”‚   â””â”€â”€ benchmark_stats.ipynb      # Statistics for performance evaluation
â”œâ”€â”€ benchmark_*.py                 # replication scripts for the three performance assessments
â”œâ”€â”€ lasso_arena_ingest.py          # script to import data from LASSO run (also contains a simple time tracker for ingestion times)
â”œâ”€â”€ pyproject.toml                 # Project dependencies (managed by uv)
â””â”€â”€ README.md                      # This file
```

*All notebooks and benchmark scripts are **self-contained** - they spin up a temporary DuckDB connection to the data in `warehouse/`.*

## Setup Instructions

### Prerequisites

| Tool | Minimum version |
|------|-----------------|
| Python | **3.12** (or newer) |
| Git   | any recent version |
| `uv` | fast dependency manager (recommended) |


### Installation

This project uses [uv](https://github.com/astral-sh/uv) for dependency management:

```bash
# Install uv
# https://docs.astral.sh/uv/getting-started/installation/
curl -LsSf https://astral.sh/uv/install.sh | sh

# Clone & enter the repo
git clone https://github.com/SoftwareObservatorium/observation-lakehouse.git
cd observation-lakehouse

# Resolve & install all dependencies
uv sync                # creates .venv and writes a lockfile
source .venv/bin/activate   # Linux/macOS
# .venv\Scripts\activate    # Windows
```

## Importing the LASSO Replication Dataset

The dataset used in the paper (~1.8GiB) is provided as a **single zip** containing raw exported data from a recent test-driven software experiment with LASSO (assessment of four code LLMs based on two benchmarks: HumanEval and MBPP)

1. **Download** the replication package  
   ðŸ‘‰ <https://swtweb.informatik.uni-mannheim.de/nexus/repository/olake/saner26tool/replication_package.zip>
2. **Unzip** its contents directly into the repository root.
3. **Run** the ingestion script to create the Iceberg catalog, load the tables and ingest the dataset above:

```bash
# depending on your local setup, run "lasso_arena_ingest.py" directly
python3 lasso_arena_ingest.py

# or using 'uv'
uv run lasso_arena_ingest.py
```

**What happens?**  
* `arena.py` parses the JSON-encoded tests, and implementations, and reads the raw observation data from large Parquet files generated by the LASSO arena.  
* `lakehouse.py` writes the data as **partitioned Parquet files** and creates the accompanying Iceberg metadata under `warehouse/db/`.  

After completion you will see the following layout based on [Hive Paritioning](https://duckdb.org/docs/stable/data/partitioning/hive_partitioning):

```
observation-lakehouse/
â”œâ”€ warehouse/
â”‚  â””â”€ db/
â”‚     â”œâ”€ observations/
â”‚     â”‚   â”œâ”€ data/ # partitioned by data_set_id and problem_id
â”‚     â”‚   â”‚   â”œâ”€ data_set_id=HumanEval/
â”‚     â”‚   â”‚   â”‚   â””â”€ problem_id=HumanEval_0_has_close_elements/
â”‚     â”‚   â”‚   â”‚        â””â”€ *.parquet # SRM for problem
â”‚     â”‚   â”‚   â””â”€ ...
â”‚     â”‚   â””â”€ metadata/          # Iceberg snapshots, manifests, ...
â”‚     â”œâ”€ code_implementations/
â”‚     â”‚   â””â”€ (same partitioning as observations)
â”‚     â””â”€ tests/
â”‚         â””â”€ (same partitioning as observations)
â””â”€ iceberg_catalog.db            # SQLite-based Iceberg catalog used by DuckDB
```

> **Note** - The `data_set_id` and `problem_id` partitions allow **partition pruning**, which is the core reason for the sub-100ms query latencies reported.


## Re-producing the Performance Benchmarks

All benchmark scripts are located in `benchmark_*.py`. They report the numbers that appear in Table 2 of the paper (average latency per problem, cold-cache run).

```bash
# SRM output view reconstruction (Q1)
uv run benchmark_srm_output_view.py

# Example: behavioural clustering (Q2 in the paper)
uv run benchmark_behavioral_clustering.py

# Full three-way join (Q3)
uv run benchmark_three_way_join.py
```

Each script produces a CSV. The notebook in `notebooks/benchmark_stats.ipynb` can be used to create descriptive statistics for these.

## Querying the Observation Lakehouse

The **analysis notebook** (`notebooks/analysis.ipynb`) walks you through the exact SQL statements:

| Section | Goal                                                                    |
|---------|-------------------------------------------------------------------------|
| **SRM output view** | Re-creates the *SRM output view*                                        |
| **Behavioural clustering** | Groups implementations by identical output trace                        |
| **Threeâ€‘way join** | Joins `observations`, `tests`, `code_implementations`                   |
| **Consensus oracle** | Computes the majority output per test case (see Behavioural clustering) |

To run the notebook:

```bash
uv run jupyter-lab

# In the browser, open notebooks/analysis.ipynb and follow the markdown cells.
```

All queries are **pure SQL** (DuckDB).

### Adding a New Dataset

1. **Analyze the table schemata** in `olake/lakehouse.py`
2. **Write an ingestion driver** in `olake/ingest/` that yields rows for the three tables (`observations`, `tests`, `code_implementations`).  The existing `arena.py` can be used as a template.


## Known Limitations

* **Serialization** - The current JSON-based stimulus/response format cannot natively represent binary streams or non-JSON-friendly objects (e.g., file handles).
* **Equivalence checking** - Complex values (e.g., exceptions with different stack traces) are treated as different unless preprocessed like we do in LASSO's [Sequence Sheet protocol](https://softwareobservatorium.github.io/web/docs/datastructures/ssn#storing-observations---serializing-outputs--protocol)
* **Real-world CI ingestion** - The provided driver only supports the LASSO arena. Test-driver extension prototypes for JUnit5 and PyTest are under development and are going to be made available in a next step.

All of these will be addressed in future releases.

## Contributing

We welcome community contributions:

1. Fork the repository.
2. Create a feature branch (`git checkout -b feature/my-new-driver`).
3. Submit a Pull Request with a clear description and, if applicable, updated benchmark numbers.

---

## Citation

If you use Observation Lakehouse in your research, you can cite our tool paper (preprint coming soon), and/or our work that introduces the continual SRC:

```bibtex
tool paper: coming soon
```

```bibtex
@article{kessel2025,
author = {Kessel, Marcus and Atkinson, Colin},
title = {Morescient {GAI} for Software Engineering},
year = {2025},
issue_date = {June 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {34},
number = {5},
issn = {1049-331X},
url = {https://doi.org/10.1145/3709354},
doi = {10.1145/3709354},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = may,
articleno = {123},
numpages = {17},
}
```

A pre-print is available at: [Arxiv](https://arxiv.org/abs/2406.04710)

---

## Contact

* **Project Lead** - *Marcus Kessel* (marcus.kessel@uni-mannheim.de)  
* **GitHub** - https://github.com/SoftwareObservatorium/observation-lakehouse  
* **Issue Tracker** - Use the GitHub *Issues* tab for bugs, feature requests, or questions.
